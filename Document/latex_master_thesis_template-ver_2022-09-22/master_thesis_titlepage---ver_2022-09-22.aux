\relax 
\AC@reset@newl@bel
\newacro{RL}[\AC@hyperlink{RL}{RL}]{Reinforcement learning}
\newacro{NN}[\AC@hyperlink{NN}{NN}]{neural network}
\newacro{DRL}[\AC@hyperlink{DRL}{DRL}]{deep reinforcement learning}
\newacro{NODE}[\AC@hyperlink{NODE}{nODE}]{neural ordinary differential equation}
\newacro{ODE}[\AC@hyperlink{ODE}{ODE}]{ordinary differential equation}
\newacro{MDP}[\AC@hyperlink{MDP}{MDP}]{markov decision process}
\newacro{TD}[\AC@hyperlink{TD}{TD}]{temporal difference}
\newacro{SARSA}[\AC@hyperlink{SARSA}{SARSA}]{State\IeC {\textendash }Action\IeC {\textendash }Reward\IeC {\textendash }State\IeC {\textendash }Action}
\newacro{DQN}[\AC@hyperlink{DQN}{DQN}]{deep Q neural networks}
\newacro{NFQ}[\AC@hyperlink{NFQ}{NFQ}]{neural fitted Q iteration}
\citation{mnih2013playing}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Introduction}{{1}{3}}
\AC@undonewlabel{acro:RL}
\newlabel{acro:RL}{{1}{3}}
\acronymused{RL}
\acronymused{RL}
\acronymused{RL}
\acronymused{RL}
\AC@undonewlabel{acro:NN}
\newlabel{acro:NN}{{1}{3}}
\acronymused{NN}
\acronymused{NN}
\AC@undonewlabel{acro:DRL}
\newlabel{acro:DRL}{{1}{3}}
\acronymused{DRL}
\acronymused{NN}
\AC@undonewlabel{acro:NODE}
\newlabel{acro:NODE}{{1}{3}}
\acronymused{NODE}
\acronymused{NN}
\AC@undonewlabel{acro:ODE}
\newlabel{acro:ODE}{{1}{3}}
\acronymused{ODE}
\acronymused{NODE}
\acronymused{NODE}
\acronymused{RL}
\acronymused{NN}
\acronymused{DRL}
\acronymused{RL}
\citation{march1991exploration}
\citation{szepesvari2009reinforcement}
\citation{sutton2018reinforcement}
\citation{bellman1957markovian}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Reinforcement Learning}{4}}
\newlabel{Reinforcement Learning}{{1.1}{4}}
\@writefile{toc}{\contentsline {paragraph}{The Reinforcement Learning Problem}{4}}
\acronymused{RL}
\acronymused{RL}
\acronymused{RL}
\@writefile{toc}{\contentsline {paragraph}{Model-based reinforcement learning}{4}}
\AC@undonewlabel{acro:MDP}
\newlabel{acro:MDP}{{1.1}{4}}
\acronymused{MDP}
\newlabel{eq:BellmanUpdateValueFunctionModelBased}{{1.1}{4}}
\newlabel{eq:BellmanUpdateValueFunctionModelBasedconvergence}{{1.2}{4}}
\newlabel{eq:utility value}{{1.3}{4}}
\newlabel{eq:optimalpoliciy}{{1.4}{4}}
\citation{sutton1988learning}
\citation{rummery1994line}
\citation{watkins1992q}
\citation{lin1992self}
\citation{sutton2018reinforcement}
\@writefile{toc}{\contentsline {paragraph}{Model-free reinforcement learning}{5}}
\AC@undonewlabel{acro:TD}
\newlabel{acro:TD}{{1.1}{5}}
\acronymused{TD}
\acronymused{TD}
\acronymused{TD}
\acronymused{TD}
\newlabel{eq:bellman update value function}{{1.5}{5}}
\AC@undonewlabel{acro:SARSA}
\newlabel{acro:SARSA}{{1.1}{5}}
\acronymused{SARSA}
\newlabel{eq:bellman update SARSA}{{1.6}{5}}
\newlabel{eq:q-learning}{{1.7}{5}}
\acronymused{SARSA}
\citation{sutton1999policy}
\citation{bertsekas1996neuro}
\citation{mnih2013playing}
\citation{silver2016mastering}
\citation{tsitsiklis1996analysis}
\citation{riedmiller2005neural}
\citation{hasselt2010double}
\citation{van2016deep}
\citation{schaul2015prioritized}
\citation{zhang2018human}
\citation{chen2022energy}
\citation{liu2019double}
\citation{he2021variational}
\citation{li2020onboard}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Deep Reinforcement Learning}{6}}
\newlabel{Deep Reinforcement Learning}{{1.2}{6}}
\acronymused{RL}
\AC@undonewlabel{acro:DQN}
\newlabel{acro:DQN}{{1.2}{6}}
\acronymused{DQN}
\AC@undonewlabel{acro:NFQ}
\newlabel{acro:NFQ}{{1.2}{6}}
\acronymused{NFQ}
\acronymused{DQN}
\acronymused{DQN}
\acronymused{DQN}
\acronymused{TD}
\citation{chen2018neural}
\citation{chen2018neural}
\citation{chen2018neural}
\citation{pontryagin1987mathematical}
\citation{chen2018neural}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Neural Ordinary Differential Equations}{7}}
\newlabel{Neural Ordinary Differential Equations}{{1.3}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Continuous Backpropagation \cite  [Figure 2]{chen2018neural}\relax }}{7}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:ContinuousBackProp}{{1.1}{7}}
\acronymused{NODE}
\newlabel{eq:NeuralNetworkAsHiddenStateDynamics}{{1.8}{7}}
\newlabel{eq:loss function}{{1.9}{7}}
\@writefile{toc}{\contentsline {paragraph}{Continuous Backpropagation}{7}}
\newlabel{eq:adjointstate}{{1.10}{7}}
\newlabel{eq:adjointstateepsilon}{{1.11}{7}}
\newlabel{eq:hiddenstatedynamicsespsilon}{{1.12}{7}}
\newlabel{eq:adjointstatedefinition}{{1.13}{7}}
\citation{chen2018neural}
\newlabel{eq:adjstate}{{1.14}{8}}
\newlabel{eq:adjointstatedynamicsdefinition}{{1.15}{8}}
\newlabel{eq:adjointstatefinalvalue}{{1.16}{8}}
\newlabel{eq:adjointstateanyvalue}{{1.17}{8}}
\@writefile{toc}{\contentsline {paragraph}{Gradients for the model parameters}{8}}
\newlabel{eq:timedynamicstheta}{{1.18}{8}}
\newlabel{eq:timedynamicst}{{1.19}{8}}
\citation{rezende2015variational}
\newlabel{eq:timedynamicscombinedaugmentedstate}{{1.20}{9}}
\newlabel{eq:fjacobian}{{1.21}{9}}
\newlabel{eq:combinedaugmentedstate}{{1.22}{9}}
\newlabel{eq:combinedaugmentedstatedynamics}{{1.23}{9}}
\newlabel{eq:combinedaugmentedstatedynamicsfinal}{{1.24}{9}}
\newlabel{eq:gradientmodelparameters}{{1.25}{9}}
\@writefile{tdo}{\contentsline {todo}{Claims from the authors}{9}}
\@writefile{toc}{\contentsline {paragraph}{Characteristics and applications of NODEs}{9}}
\pgfsyspdfmark {pgfid1}{20721626}{20640979}
\pgfsyspdfmark {pgfid4}{36118906}{20653267}
\pgfsyspdfmark {pgfid5}{38232442}{20429353}
\citation{dupont2019augmented}
\citation{lin2018resnet}
\citation{jian2016deep}
\citation{lin2018resnet}
\citation{sander2022residual}
\citation{dupont2019augmented}
\citation{dupont2019augmented}
\citation{barto1983neuronlike}
\citation{gymref}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Reinforcement Learning with neural ODEs}{10}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Reinforcement Learning with neural ODEs}{{2}{10}}
\acronymused{NODE}
\acronymused{DQN}
\acronymused{ODE}
\acronymused{ODE}
\acronymused{DRL}
\acronymused{NODE}
\acronymused{NODE}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces nODE architecture \cite  [Figure 2]{dupont2019augmented}\relax }}{10}}
\newlabel{fig:nODEArchitecture}{{2.1}{10}}
\@writefile{toc}{\contentsline {paragraph}{Hypothesis}{10}}
\acronymused{NODE}
\@writefile{tdo}{\contentsline {todo}{Dual model, prioritised experience replay, epsilon greedy}{10}}
\@writefile{toc}{\contentsline {paragraph}{Model Set up}{10}}
\pgfsyspdfmark {pgfid6}{11012894}{20308514}
\pgfsyspdfmark {pgfid9}{36118906}{20320802}
\pgfsyspdfmark {pgfid10}{38232442}{20096888}
\@writefile{toc}{\contentsline {paragraph}{Problem Setting}{10}}
\acronymused{DRL}
\citation{hasselt2010double}
\citation{schaul2015prioritized}
\newlabel{tab:model-hyperparameter}{{\caption@xref {tab:model-hyperparameter}{ on input line 355}}{11}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Model Hyperparameter\relax }}{11}}
\@writefile{toc}{\contentsline {paragraph}{Set up}{11}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Results}{12}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Results}{{3}{12}}
\bibstyle{plain}
\bibdata{refs}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Conclusion}{13}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Conclusion}{{4}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Outlook}{13}}
\newlabel{Outlook}{{4.1}{13}}
\bibcite{barto1983neuronlike}{1}
\bibcite{bellman1957markovian}{2}
\bibcite{bertsekas1996neuro}{3}
\bibcite{gymref}{4}
\bibcite{chen2018neural}{5}
\bibcite{chen2022energy}{6}
\bibcite{dupont2019augmented}{7}
\bibcite{hasselt2010double}{8}
\bibcite{he2021variational}{9}
\bibcite{jian2016deep}{10}
\bibcite{li2020onboard}{11}
\bibcite{lin2018resnet}{12}
\bibcite{lin1992self}{13}
\bibcite{liu2019double}{14}
\bibcite{march1991exploration}{15}
\bibcite{mnih2013playing}{16}
\bibcite{pontryagin1987mathematical}{17}
\bibcite{rezende2015variational}{18}
\bibcite{riedmiller2005neural}{19}
\bibcite{rummery1994line}{20}
\bibcite{sander2022residual}{21}
\bibcite{schaul2015prioritized}{22}
\bibcite{silver2016mastering}{23}
\bibcite{sutton1988learning}{24}
\bibcite{sutton2018reinforcement}{25}
\bibcite{sutton1999policy}{26}
\bibcite{szepesvari2009reinforcement}{27}
\bibcite{tsitsiklis1996analysis}{28}
\bibcite{van2016deep}{29}
\bibcite{watkins1992q}{30}
\bibcite{zhang2018human}{31}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Experimental Set Up}{17}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Balancing Pole}{17}}
\newlabel{Balancing Pole Appendix}{{A.1}{17}}
\newlabel{tab:actions-bp}{{\caption@xref {tab:actions-bp}{ on input line 393}}{17}}
\newlabel{tab:observations-bp}{{\caption@xref {tab:observations-bp}{ on input line 405}}{17}}
